{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80e5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2174bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "df = pd.read_csv('emails (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20256add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-step: Create numerical features for analysis\n",
    "df['text_len'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc9df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " text          0\n",
      "spam          0\n",
      "text_len      0\n",
      "word_count    0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      " text            str\n",
      "spam          int64\n",
      "text_len      int64\n",
      "word_count    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- TASK 1: Identify Data Quality Issues ---\n",
    "# Check for missing values and data types\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "print(\"\\nData Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc57923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TASK 2: Missing Value Strategy ---\n",
    "# We will simulate a missing value and then impute it using the MEAN\n",
    "df_missing = df.copy()\n",
    "df_missing.loc[0:5, 'text_len'] = np.nan # Simulate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89cd472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputed missing values with mean: 1557.746941628801\n"
     ]
    }
   ],
   "source": [
    "# Strategy: Imputation with Mean\n",
    "mean_value = df_missing['text_len'].mean()\n",
    "df_missing['text_len'] = df_missing['text_len'].fillna(mean_value)\n",
    "print(f\"\\nImputed missing values with mean: {mean_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8764e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TASK 3: Detect and Handle Outliers (IQR) ---\n",
    "Q1 = df['text_len'].quantile(0.25)\n",
    "Q3 = df['text_len'].quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94973b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e27991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers removed. Rows reduced from 5728 to 5355\n"
     ]
    }
   ],
   "source": [
    "# Filtering outliers\n",
    "df_clean = df[(df['text_len'] >= lower_bound) & (df['text_len'] <= upper_bound)].copy()\n",
    "print(f\"\\nOutliers removed. Rows reduced from {len(df)} to {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c135f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TASK 4: Normalize Numerical Features ---\n",
    "# Min-Max Normalization (Scaling to 0-1)\n",
    "df_clean['len_minmax'] = (df_clean['text_len'] - df_clean['text_len'].min()) / (df_clean['text_len'].max() - df_clean['text_len'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a0bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score Normalization (Standardization)\n",
    "df_clean['len_zscore'] = (df_clean['text_len'] - df_clean['text_len'].mean()) / df_clean['text_len'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5647efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TASK 5: PCA and Explained Variance ---\n",
    "# Standardize the features before PCA\n",
    "features = ['text_len', 'word_count']\n",
    "x = StandardScaler().fit_transform(df_clean[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb9d3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8986773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2205f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.99151578 0.00848422]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81fdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
